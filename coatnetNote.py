from sys import argv
import os
import numpy as np
from pytorchCoatnet import CoAtNet
import torch
from torch.utils.data import DataLoader, random_split
from melDataset import melDataset
from keyLabel import keyLabel
from config import config

'''
coatnetNote.py

This script trains a neural network to classify the notes of a keyboard. It uses the mel spectrograms generated by librosaPeaks.py as the training data.

We chose to use CoAtNet as the neural network architecture for this project because the paper we based our work off reccomended it and 
it is a lightweight convolutional transformer that is efficient for image classification tasks. CoatNet is a convolutional transformer that 
uses a convolutional backbone to extract features from the input image and a transformer to model the relationships between the features.

Our implementation of CoatNet was created by chinhsuanwu and the github repo can be found at https://github.com/chinhsuanwu/coatnet-pytorch.

Usage:
    python3 coatnetNote.py <training_data_directory>

Arguments:
    training_data_directory: The directory containing the mel spectrograms of the training data. The mel spectrograms should be in .npy format.

'''

model = CoAtNet(config.img_size[0:2], 3, config.num_blocks, config.channels, num_classes=config.num_classes)

loss_fn = torch.nn.CrossEntropyLoss()
optim = torch.optim.AdamW(
    model.parameters(),
    lr=config.lr,
    weight_decay=config.weight_decay
)

label_encoder = keyLabel()



'''
load_training_data(training_data_directory)

This function loads the training data from the training data directory. It loads the mel spectrograms and their corresponding labels.

Arguments:
    training_data_directory: The directory containing the mel spectrograms of the training data. The mel spectrograms should be in .npy format.

Returns:
    A list of tuples containing the mel spectrograms and their corresponding labels.
    The mel spectrograms are NumPy arrays and the labels are integers.
'''
def load_training_data(training_data_directory):
    mel_spectrograms = []
    labels = []
    for note_folder in os.listdir(training_data_directory):
        if os.path.isdir(os.path.join(training_data_directory, note_folder)):
            note_path = os.path.join(training_data_directory, note_folder)
            for file_name in os.listdir(note_path):
                if file_name.endswith('.npy'):  # assuming mel spectrograms are in .npy format
                    file_path = os.path.join(note_path, file_name)
                    mel_spectrogram = np.load(file_path)  # load mel spectrogram using NumPy
                    # Reshape or preprocess mel spectrogram if needed
                    mel_spectrograms.append(mel_spectrogram)
                    label = str(note_folder)[0]
                    print("Loaded note ", label, " with size ", mel_spectrogram.shape)
                    labels.append(label)  # use the folder name as the label for the note
    
    # First collect all labels, then fit the encoder
    print(f"Found {len(set(labels))} unique labels: {set(labels)}")  # Debug labels
    label_encoder.fit(labels)  # This is the critical missing step
    labels = label_encoder.transform(labels)
    
    return list(zip(mel_spectrograms, labels))



'''
load_testing_data(testing_file_path)

This function loads the testing data from the testing file path. It loads the mel spectrograms and their corresponding labels.

Arguments:
    testing_file_path: The path to the testing file. The testing file should contain the mel spectrograms and their corresponding labels.

Returns:
    A list of tuples containing the mel spectrograms and their corresponding labels.
    The mel spectrograms are NumPy arrays and the labels are integers.
'''
def load_testing_data(testing_file_path):
    # Load mel spectrograms and corresponding labels from the testing file
    testing_mel_spectrograms = []
    testing_labels = []

    # Iterate through each file in the testing file path
    for file_name in os.listdir(testing_file_path):
        if file_name.endswith('.npy'):  # Check if the file is a numpy file
            file_path = os.path.join(testing_file_path, file_name)
            mel_spectrogram = np.load(file_path)  # Load mel spectrogram using NumPy
            # Print shape for debugging
            print("Loaded testing mel spectrogram:", file_path)
            print("Loaded testing mel spectrogram shape:", mel_spectrogram.shape)
            # Reshape or preprocess mel spectrogram if needed
            testing_mel_spectrograms.append(mel_spectrogram)
            # Extract label from the file name (assuming the label is in the file name)
            label = file_name.split('_')[0]  # Extract the label from the file name
            print("Test Label: ", label)
            testing_labels.append(label)  # Append the label to the list of labels
    testing_labels = label_encoder.transform(testing_labels)  # Convert labels to integers
    return list(zip(testing_mel_spectrograms, testing_labels))



'''
train_epoch(model, data_loader)

This function trains the model for one epoch using the data loader.

Arguments:
    model: The neural network model.
    data_loader: The data loader containing the training data.

Returns:
    The training loss and the number of correct predictions.
'''
def train_epoch(model, data_loader):
    model.train()
    train_loss, train_correct = 0.0, 0

    for step, batch in enumerate(data_loader):
        optim.zero_grad()
        x, y = batch
        x = x.unsqueeze(1).repeat(1, 3, 1, 1)  # Convert to 3-channel
        print(f'\r{step+1}/{len(data_loader)}', end='')
        logits = model(x.float())
        loss = loss_fn(logits, y)
        train_loss += loss.item()
        loss.backward()
        optim.step()

        # calculate accuracy
        preds = torch.argmax(logits, dim=1).flatten()
        correct_preds_n = (preds == y).cpu().sum().item()
        train_correct += correct_preds_n
    return train_loss, train_correct




'''
valid_epoch(model, data_loader)

This function evaluates the model for one epoch using the data loader.

Arguments:
    model: The neural network model.
    data_loader: The data loader containing the validation data.

Returns:
    The validation loss and the number of correct predictions.
'''
def valid_epoch(model, data_loader):
    model.eval()
    val_loss, val_correct = 0.0, 0
    
    with torch.no_grad():
        for step, batch in enumerate(data_loader):
            x, y = batch
            x = x.unsqueeze(1).repeat(1, 3, 1, 1)  # Convert to 3-channel
            print(f'\r{step+1}/{len(data_loader)}', end='')
            logits = model(x.float()) 
            loss = loss_fn(logits, y)
            val_loss += loss.item()
            preds = torch.argmax(logits, dim=1).flatten()
            correct_preds_n = (preds == y).cpu().sum().item()
            val_correct += correct_preds_n
    
    return val_loss, val_correct




'''
main()

This is the main function that trains the neural network using the training data and evaluates it using the testing data.
This function uses KFold cross-validation to split the training data into training and validation sets.

Arguments:
    None

Returns:
    None
'''
def main():
    if (len(argv) > 1):
        training_data_directory = argv[1]
    else:
        training_data_directory = 'training_data_2'

    # Load and prepare data
    training_data = load_training_data(training_data_directory)
    dataset = melDataset(training_data)

    # Split dataset into train and validation sets (80/20 split)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)

    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    for epoch in range(config.epochs):
        torch.cuda.empty_cache()
        print('---train:')    
        train_loss, train_correct = train_epoch(model, train_loader)
        print('\n---eval:')
        val_loss, val_correct = valid_epoch(model, val_loader)
        
        train_loss = train_loss / len(train_loader.dataset)
        train_acc = train_correct / len(train_loader.dataset) * 100
        val_loss = val_loss / len(val_loader.dataset)
        val_acc = val_correct / len(val_loader.dataset) * 100
        
        print('\n---status:')
        print(f"\tEpoch:{epoch + 1}/{config.epochs}")
        print(f"\tAverage Training Loss:{train_loss:.4f}, Average Validation Loss:{val_loss:.4f}")
        print(f"\tAverage Training Acc {train_acc:.2f}%, Average Validation Acc {val_acc:.2f}%\n")
        
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

    torch.save(model.state_dict(), "model_weights_personal.pth")
    np.save('history.npy', history)

if __name__ == "__main__":
    main()